{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross-validation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxX7ExuOrHPi"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "if not os.path.isdir(\"source\"):\n",
        "   with zipfile.ZipFile(\"source.zip\", 'r') as zip_ref:\n",
        "      zip_ref.extractall(\".\")\n",
        "\n",
        "if not os.path.isdir(\"data\"):\n",
        "   with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
        "      zip_ref.extractall(\".\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfSDQimsqx3W"
      },
      "source": [
        "import gzip\n",
        "import sys\n",
        "import urllib\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "import code\n",
        "import tensorflow.python.platform\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.backend as backend\n",
        "import tensorflow_addons as tfa\n",
        "import source.mask_to_submission as submission_maker\n",
        "import source.constants as cst\n",
        "import source.images as images\n",
        "from sklearn.model_selection import KFold\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brhj66-IaADZ"
      },
      "source": [
        "def recall(y, predictions):\n",
        "    true_positives = backend.sum(backend.round(backend.clip(y * predictions, 0, 1)))\n",
        "    possible_positives = backend.sum(backend.round(backend.clip(y, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + backend.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y, predictions):\n",
        "    true_positives = backend.sum(backend.round(backend.clip(y * predictions, 0, 1)))\n",
        "    predicted_positives = backend.sum(backend.round(backend.clip(predictions, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + backend.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_metric(y, predictions):\n",
        "    pre = precision(y, predictions)\n",
        "    rec = recall(y, predictions)\n",
        "    return 2 * ((pre * rec) / (pre + rec + backend.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUsdwqI0zAGz"
      },
      "source": [
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, alpha=1.0, gamma=2.0, reduction=tf.keras.losses.Reduction.AUTO, name='focal_loss'):\n",
        "        super().__init__(reduction=reduction, name=name)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        y_true = tf.expand_dims(y_true, axis=-1)\n",
        "\n",
        "        bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "        pt = tf.math.exp(-bce_loss)\n",
        "\n",
        "        return self.alpha * tf.math.pow(1-pt, self.gamma) * bce_loss -\\\n",
        "               (1-self.alpha) * tf.math.pow(pt, self.gamma) * tf.math.log(1-pt)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRw8F76Yre3Q"
      },
      "source": [
        "def get_unet(alpha=1.0, gamma=2.0):\n",
        "    inputs = layers.Input((200, 200, 3), name=\"input_layer\")\n",
        "    drop1 = layers.Dropout(cst.DROPOUT_PROBABILITY)(inputs)\n",
        "    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(drop1)\n",
        "    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = layers.MaxPooling2D((2, 2), (2, 2))(conv1)\n",
        "    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = layers.MaxPooling2D((2, 2), (2, 2))(conv2)\n",
        "    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    pool3 = layers.MaxPooling2D((2, 2), (2, 2))(conv3)\n",
        "    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    pool4 = layers.MaxPooling2D((2, 2), (2, 2))(conv4)\n",
        "    conv5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    up6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(conv5)\n",
        "    cropped6 = tf.image.resize_with_crop_or_pad(conv4, up6.shape[1], up6.shape[2])\n",
        "    conc6 = layers.concatenate([up6, cropped6], axis=3)\n",
        "    conv6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conc6)\n",
        "    conv6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    up7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
        "    cropped7 = tf.image.resize_with_crop_or_pad(conv3, up7.shape[1], up7.shape[2])\n",
        "    conc7 = layers.concatenate([up7, cropped7], axis=3)\n",
        "    conv7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conc7)\n",
        "    conv7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    up8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv7)\n",
        "    cropped8 = tf.image.resize_with_crop_or_pad(conv2, up8.shape[1], up8.shape[2])\n",
        "    conc8 = layers.concatenate([up8, cropped8], axis=3)\n",
        "    conv8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conc8)\n",
        "    conv8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)\n",
        "    up9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv8)\n",
        "    cropped9 = tf.image.resize_with_crop_or_pad(conv1, up9.shape[1], up9.shape[2])\n",
        "    conc9 = layers.concatenate([up9, cropped9], axis=3)\n",
        "    conv9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conc9)\n",
        "    drop10 = layers.Dropout(cst.DROPOUT_PROBABILITY)(conv9)\n",
        "    conv10 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(drop10)\n",
        "    conv10 = layers.Conv2D(1, (1, 1), activation='sigmoid', name=\"output_layer\")(conv10)\n",
        "\n",
        "    unet = tf.keras.Model(inputs=[inputs], outputs=[conv10])\n",
        "    unet.compile(optimizer='adam', loss=FocalLoss(alpha=alpha, gamma=gamma), metrics=[f1_metric, 'accuracy'])\n",
        "    return unet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro7fHSkkrfn6"
      },
      "source": [
        "def train_unet(unet, train_data, train_labels):\n",
        "\n",
        "    output_shape = unet.get_layer(\"output_layer\").output_shape\n",
        "    margin = int((train_labels.shape[1] - output_shape[1]) / 2)\n",
        "    right_margin = int(output_shape[1] + margin)\n",
        "    train_labels = train_labels[:,margin:right_margin,margin:right_margin]\n",
        "\n",
        "    unet.fit(train_data, train_labels, epochs=20, validation_split=0.0, batch_size=cst.BATCH_SIZE, callbacks=[], verbose=0)\n",
        "\n",
        "    return mean_train, std_train\n",
        "\n",
        "def evaluate_unet(unet, test_data, test_labels):\n",
        "    output_shape = unet.get_layer(\"output_layer\").output_shape\n",
        "    margin = int((test_labels.shape[1] - output_shape[1]) / 2)\n",
        "    right_margin = int(output_shape[1] + margin)\n",
        "    test_labels = test_labels[:,margin:right_margin,margin:right_margin]\n",
        "\n",
        "    return unet.evaluate(test_data, test_labels, batch_size=cst.BATCH_SIZE, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ruYBlLaYpLO"
      },
      "source": [
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits)\n",
        "\n",
        "alphas = [0.6, 0.75, 0.9, 1.0]\n",
        "gammas = [1.0, 2.0, 5.0]\n",
        "\n",
        "ops = len(alphas)*len(gammas)*n_splits\n",
        "results = np.zeros((len(alphas), len(gammas)), dtype=np.float64)\n",
        "\n",
        "train_data_filename = cst.TRAIN_DIR + 'images/'\n",
        "train_labels_filename = cst.TRAIN_DIR + 'groundtruth/' \n",
        "\n",
        "# Extract it into numpy arrays.\n",
        "train_data, mean_train, std_train = images.load_training(train_data_filename, cst.TRAINING_SIZE)\n",
        "train_labels = images.load_groundtruths(train_labels_filename, cst.TRAINING_SIZE)\n",
        "\n",
        "print(\"DATA SHAPE \" + str(train_data.shape))\n",
        "print(\"TRAIN_LABELS SHAPE \" + str(train_labels.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hav7dOTNUn6N"
      },
      "source": [
        "start = time.time()\n",
        "ops_done=0\n",
        "\n",
        "print(\"fold\\talpha\\tgamma\\tf1\\tcompl\\telapsed\\tleft\")\n",
        "\n",
        "for a, alpha in enumerate(alphas):\n",
        "    for g, gamma in enumerate(gammas):\n",
        "        intermediates = []\n",
        "        cpt = 0\n",
        "        for train, test in kf.split(train_data):\n",
        "            X_train, X_test = train_data[train], train_data[test]\n",
        "            Y_train, Y_test = train_labels[train], train_labels[test]\n",
        "\n",
        "            unet = get_unet(alpha, gamma)\n",
        "            train_unet(unet, X_train, Y_train)\n",
        "\n",
        "            fbeta = evaluate_unet(unet, X_test, Y_test)[1]\n",
        "\n",
        "            if fbeta > 1:\n",
        "                fbeta = 0.\n",
        "\n",
        "            intermediates.append(fbeta)\n",
        "\n",
        "            ops_done += 1\n",
        "            elapsed = time.time()-start\n",
        "            left = ops*elapsed/ops_done - elapsed if ops_done != 0 else -1\n",
        "\n",
        "            print(\"{}\\t{:.2f}\\t{:.2f}\\t{:.4f}\\t{:.2f}%\\t{}s\\t{}s\".format(cpt, alpha, gamma, fbeta, ops_done/ops*100, int(elapsed), int(left)))\n",
        "            cpt += 1\n",
        "            del unet\n",
        "        \n",
        "        results[a][g] = np.median(intermediates)\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iO5imQKUagj"
      },
      "source": [
        "print(\"alpha\\tgamma\\tf1_score\")\n",
        "for a, alpha in enumerate(alphas):\n",
        "    for g, gamma in enumerate(gammas):\n",
        "        print(\"{:.2f}\\t{:.2f}\\t{:.4f}\".format(alpha, gamma, results[a][g]))\n",
        "\n",
        "a, g = np.unravel_index(np.argmax(results, axis=None), results.shape)\n",
        "\n",
        "print(\"\\nBest result with:\")\n",
        "print(\"alpha\\tgamma\\tf1_score\")\n",
        "print(\"{:.2f}\\t{:.2f}\\t{:.4f}\".format(alphas[a], gammas[g], results[a][g]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}