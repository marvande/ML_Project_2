{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da-ke2bssx8l"
   },
   "source": [
    "# ML Project 2: Segmentation of aerial images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9idtyadbsx8n"
   },
   "source": [
    "Instructions: \n",
    "- exploratory data analysis to understand your dataset and your features\n",
    "- feature processing and engineering to clean your dataset and extract more meaningful information\n",
    "- implement and use machine learning methods on real data\n",
    "- analyze your model and generate predictions using those methods and report your findings\n",
    "\n",
    "**Submission deadline**. Dec 17th, 2020 (at 16:00 afternoon, sharp)\n",
    "\n",
    "Deliverables at a glance. (More details and grading criteria further down)\n",
    "- Written Report. You will write a maximum 4 page PDF report on your \f",
    "ndings, using LaTeX.\n",
    "- Code. In Python. External libraries are allowed, if properly cited.\n",
    "- Competitive Part. To give you immediate feedback and a fair ranking, we use the competition platform AIcrowd.com to score your predictions. You can submit whenever and almost as many times as you like, up until the final submission deadline.\n",
    "\n",
    "**Goal**: For this problem, we provide a set of satellite/aerial images acquired from GoogleMaps. We also provide ground-truth images where each pixel is labeled as {road, background}. Your goal is to train a classifier to segment roads in these images, i.e. assign a label {road=1, background=0} to each pixel. Please see detailed instructions on the course github.\n",
    "\n",
    "Good summary:https://neptune.ai/blog/image-segmentation-in-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7czJy2GPsx8n"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeEtZkQdvWFW"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"colab.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0pNUyiFpsx8n",
    "outputId": "c64b12f7-2e45-4432-cf2a-2382d2a54276"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "from torchvision.models import vgg16_bn\n",
    "import torchvision.transforms as transforms\n",
    "from skimage import measure\n",
    "from numpy import linalg as LA\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# helper functions from lab (load csv, create prediction, etc)\n",
    "from helper_functions.helper_functions import *\n",
    "# script to reconstruct an image from the sample submission file\n",
    "from helper_functions.submission_to_mask import *\n",
    "\n",
    "#script to make a submission file from a binary image:\n",
    "from helper_functions.mask_to_submission import *\n",
    "\n",
    "# Baseline for machine learning project on road segmentation.\n",
    "#from helper_functions.tf_aerial_images import *\n",
    "\n",
    "# Baseline for machine learning project on road segmentation.\n",
    "from helper_functions.helper_Marijn import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6yOJcMCsx8o"
   },
   "source": [
    "Set CUDA if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIW_mQR-sx8o",
    "outputId": "1f2d79f3-5a94-4b57-bbf1-cbcd8f937b32"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9S6m0fvsx8o"
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "# Concatenate an image and its groundtruth\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH0X-Wlisx8o"
   },
   "source": [
    "## Data pre-processing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4wxLswYsx8o"
   },
   "source": [
    "File descriptions:\n",
    "- `training` - the training set consisting of images with their ground truth\n",
    "- `test_set_images` - the test set\n",
    "- `sampleSubmission.csv` - a sample submission file in the correct format\n",
    "- `mask_to_submission.py` - script to make a submission file from a binary image\n",
    "- `submission_to_mask.py` - script to reconstruct an image from the sample submission file\n",
    "- `tf_aerial_images.py` - Baseline for machine learning project on road segmentation. This simple baseline consits of a CNN with two convolutional+pooling layers with a soft-max loss\n",
    "\n",
    "\n",
    "The sample submission file contains two columns: \n",
    "- first column corresponds to the image id followed by the x and y top-left coordinate of the image patch (16x16 pixels)\n",
    "- second column is the label assigned to the image patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45EkCJpesx8o"
   },
   "source": [
    "### Paths to data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POru6MhMsx8o"
   },
   "outputs": [],
   "source": [
    "path = Path('data/training/')\n",
    "path_test = Path('data/test_set_images/')\n",
    "\n",
    "if not (path /'images').exists():\n",
    "    (path /'images').mkdir()\n",
    "    \n",
    "if not (path / 'groundtruth').exists():\n",
    "    (path / 'groundtruth').mkdir()\n",
    "\n",
    "path_train = path /'images'\n",
    "path_GT = path / 'groundtruth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ9aeqmYsx8o",
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Pixel range: \n",
    "Look at pixel values for GT and images. GT are supposed to be binary labels (0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uypg7yXssx8o"
   },
   "source": [
    "Get image file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwi6bVvysx8o",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "f_train_names = get_image_files(path_train)\n",
    "f_gt_names = get_image_files(path_GT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgsy3Ns4sx8o"
   },
   "source": [
    "Look at pixel range for one GT and image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8O6RJMUnsx8o"
   },
   "outputs": [],
   "source": [
    "mask = open_mask(f_gt_names[0])\n",
    "img = open_image(f_train_names[0])\n",
    "img_gt = open_image(f_gt_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "oAeqLedisx8o",
    "outputId": "7daeb3e4-684b-429e-839f-2d84b6019d08"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3, figsize = (5, 2))\n",
    "axs[0].hist(img.data[0])\n",
    "axs[0].set_title('Raw image')\n",
    "axs[1].hist(mask.data[0])\n",
    "axs[1].set_title('Mask')\n",
    "axs[2].hist(img_gt.data[0])\n",
    "axs[2].set_title('gt');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CxXazF_sx8p"
   },
   "source": [
    "Comment: problem is that GT is not binary label, see some values between 0-1 will need to apply a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5EoTNYWsx8p"
   },
   "source": [
    "### Create patches: \n",
    "Create 16x16 patches of data and masks:\n",
    "\n",
    "https://medium.com/analytics-vidhya/a-simple-cloud-detection-walk-through-using-convolutional-neural-network-cnn-and-u-net-and-bc745dda4b04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pe4SywY6sx8p"
   },
   "outputs": [],
   "source": [
    "f_train_names = get_image_files(path_train)\n",
    "f_gt_names = get_image_files(path_GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9R8rW-9sx8p",
    "outputId": "1d0c959f-21eb-4eb4-9b2c-06d3c2a1fa03"
   },
   "outputs": [],
   "source": [
    "# Extract patches from input images\n",
    "patch_size = 16  # each patch is 16*16 pixels\n",
    "\n",
    "# Number of images to extract patches\n",
    "N = 30\n",
    "img_patches = [\n",
    "    img_crop(mpimg.imread(img), patch_size, patch_size)\n",
    "    for img in f_train_names[:N]\n",
    "]\n",
    "gt_patches = [\n",
    "    img_crop(mpimg.imread(img), patch_size, patch_size) for img in f_gt_names[:N]\n",
    "]\n",
    "\n",
    "# Linearize list of patches\n",
    "img_patches = np.asarray([\n",
    "    img_patches[i][j] for i in range(len(img_patches))\n",
    "    for j in range(len(img_patches[i]))\n",
    "])\n",
    "gt_patches = np.asarray([\n",
    "    gt_patches[i][j] for i in range(len(gt_patches))\n",
    "    for j in range(len(gt_patches[i]))\n",
    "])\n",
    "patch_shape = img_patches.shape[1]\n",
    "print(f'Number of patches: {len(img_patches)} created from {N} images')\n",
    "print(f'Shape of patches: {patch_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWPcqCLCsx8q"
   },
   "source": [
    "Create RGB PIL image from patches and save them. To meet the fastai requirements, we should organize our data into `patches/images` and `patches/labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHgD3WxLsx8q"
   },
   "outputs": [],
   "source": [
    "if not (path/'patches/images').exists():\n",
    "    (path/'patches/images').mkdir()\n",
    "\n",
    "if not (path/'patches/labels').exists():\n",
    "    (path/'patches/labels').mkdir()\n",
    "    \n",
    "path_data = Path('data/training/patches')\n",
    "path_lbl = path_data/'labels'\n",
    "path_img = path_data/'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEzlDoY5sx8q"
   },
   "source": [
    "Create RGB 16x16 patches of images and saves them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5T0NGggsx8q"
   },
   "outputs": [],
   "source": [
    "for i in range(len(img_patches)):\n",
    "    rgb_patch = Image.fromarray((256 * img_patches[i]).astype(np.uint8), 'RGB')\n",
    "    rgb_patch.save(path_img / f'patch_{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NilNEs4Asx8q"
   },
   "source": [
    "#### Create binary masks: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKULv6Evsx8q"
   },
   "source": [
    "Convert the ground truth images to values 0 (no road) and 1 (road) and store them into a folder called `pacthes/labels`. In pillow, all images stored in 0-255 range so actually we give road the label (255). This will be corrected in the tf model later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArTYC0vssx8q"
   },
   "source": [
    "Show what this looks like for one mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "fYkwDM5csx8q",
    "outputId": "9b655644-c074-4058-cf73-01a3aab31d2f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "im = open_image(f_gt_names[0])\n",
    "print(f'Value range in GT:{np.unique(im.data.numpy())}')\n",
    "# mask:\n",
    "img = mpimg.imread(f_gt_names[0])\n",
    "im_mask = Image.fromarray((np.where(img > 0.5, 1, 0) * 255).astype(np.uint8),\n",
    "                          'L')\n",
    "print(f'Value range in mask:{np.unique(im_mask)}')\n",
    "im.show(ax[0])\n",
    "ax[0].set_title('GT')\n",
    "ax[1].imshow(np.asarray(im_mask), cmap='Greys_r')\n",
    "ax[1].set_title('Binary mask');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J48oNDNdsx8q"
   },
   "source": [
    "Repeat for all masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfwxUCcvsx8q"
   },
   "outputs": [],
   "source": [
    "threshold = 0.5 \n",
    "for i in range(len(gt_patches)):\n",
    "    img = gt_patches[i]\n",
    "    # Assign label \"road\" to all pixels above a threshold:\n",
    "    im = Image.fromarray((np.where(img > threshold, 1, 0) * 255).astype(np.uint8))\n",
    "    # save images:\n",
    "    im.save(path_lbl / f'patch_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REQRNcXpsx8q",
    "outputId": "8a19f77f-62c3-4748-dd29-f9a4309377f3"
   },
   "outputs": [],
   "source": [
    "# get images and labels filenames\n",
    "img_names = get_image_files(path_img)\n",
    "lbl_names = get_image_files(path_lbl)\n",
    "print(len(img_names), len(lbl_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzbnG-BDsx8q"
   },
   "outputs": [],
   "source": [
    "def concatenate_mask(list_masks):\n",
    "    \"\"\"list_masks of size (625, 16, 16) where the image \n",
    "    is assembled column after column so first 25 elements \n",
    "    are the first elements in the first column starting from pos [0,0] to [25,0]\n",
    "    masks are numpy of size (16,16)\"\"\"\n",
    "    z = np.zeros((400, 400))\n",
    "    for i in range(25):\n",
    "        columns = np.concatenate(list_masks[0 + i * 25:25 + i * 25], axis=0)\n",
    "        z[:, 0 + i * 16:16 + i * 16] = columns\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "Dkk6mj3csx8q",
    "outputId": "17be72f8-de25-49bb-9365-41ada346d28a"
   },
   "outputs": [],
   "source": [
    "plt.imshow(concatenate_mask(gt_patches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmVold8Isx8r"
   },
   "source": [
    "### Data bunch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR0qswxxsx8r"
   },
   "source": [
    "Load images and labels into tensors. Instead of using the entire set of images as training, we will let the library divide them into training (80%) and validation (20%) sets. Also, we will use some data augmentation. Data augmentation is a technique to increase the number of training samples by applying some random transformations like rotation, flipping, warp, and others.  Data Bunch keeps track of the samples and respective labels and, in the case of image segmentation, also merges both for a fast visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJOI3gd5sx8r"
   },
   "outputs": [],
   "source": [
    "# Classes for segmentation with 0,255 labels:\n",
    "class SegLabelListCustom(SegmentationLabelList):\n",
    "    def open(self, fn):\n",
    "        return open_mask(fn, div=True)\n",
    "class SegItemListCustom(SegmentationItemList):\n",
    "    _label_cls = SegLabelListCustom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6fqxtD6sx8r"
   },
   "source": [
    "Data loader, normalize to imagenet statistics as unet pretrained on imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zh2-3cdIHdS3",
    "outputId": "a9964bd1-8574-4d91-a751-68abbd3e00cf"
   },
   "outputs": [],
   "source": [
    "free = gpu_mem_get_free_no_cache()\n",
    "# the max size of the test image depends on the available GPU RAM \n",
    "if free > 8200: bs=16  \n",
    "else:           bs=8\n",
    "print(f\"using bs={bs}, size={size}, have {free}MB of GPU RAM free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2WRQcjnsx8r",
    "outputId": "94648680-50a2-4127-fcb1-999e66abc189"
   },
   "outputs": [],
   "source": [
    "print(f'Batch size:{bs}')\n",
    "print(f'Patch shape:{patch_shape}')\n",
    "\n",
    "src = (SegItemListCustom.from_folder(\n",
    "    path_img).split_by_rand_pct().label_from_func(\n",
    "        lambda x: path_lbl / x.relative_to(path_img), classes=['rest',\n",
    "                                                               'road']))\n",
    "data = (src.transform(get_transforms(flip_vert=True),\n",
    "                      size=patch_shape,\n",
    "                      tfm_y=True).databunch(bs=bs).normalize(imagenet_stats))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "jP01gmC_sx8r",
    "outputId": "e968e13a-ec35-4324-f355-a578dafe7512"
   },
   "outputs": [],
   "source": [
    "data.show_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jluSw7Y-sx8r"
   },
   "source": [
    "Look at a few images with their masks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "CHJt6f90sx8r",
    "outputId": "4d13188b-305b-4273-9d44-010caa7f1ab3"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10,2, figsize=(10,6))\n",
    "\n",
    "for i in range(10):\n",
    "    im = data.valid_ds.x[i]\n",
    "    mask = data.valid_ds.y[i]\n",
    "    label = patch_to_label(mask.data.numpy())\n",
    "    print(f'Label:{label}')\n",
    "    ax[i,0].imshow(im.data.numpy()[0])\n",
    "    ax[i,1].imshow(mask.data.numpy()[0], cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JBtqreHsx8r"
   },
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8wv5YRbsx8r",
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Model:\n",
    "pre-trained ResNet 34 version of the U-Net, that has 34 layers in the contracting path. To create it, we will define a accuracy function, to measure the performance of the mode, the weight decay (regularization to avoid overfitting of the model) value and the learning rate (rate that will be multiplied to the gradient to adjust parameters during back-propagation step).\n",
    "\n",
    "accuracy in an image segmentation problem is the same as that in any classification problem. `Accuracy = no. of correctly classified pixels / total no. of pixels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOYiJPXY6o6N"
   },
   "outputs": [],
   "source": [
    "lr  = 1e-3\n",
    "def do_fit(save_name, lrs=lr, pct_start=0.9):\n",
    "    \"\"\"\n",
    "    do_fit: fits during ? epochs with feature loss. \n",
    "    \"\"\"\n",
    "    learn.fit_one_cycle(30,\n",
    "                        lrs,\n",
    "                        pct_start=pct_start\n",
    "                        )\n",
    "    learn.save(save_name, return_path=True)\n",
    "    learn.show_results(rows=1, imgsize=10)\n",
    "    learn.recorder.plot_losses()\n",
    "    learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "8d8884f3a2e3401485e67ec7b8f87a7c",
      "1c82447708ad49cb9db6c0731f2e6e89",
      "9af9506386cd4d94b2dcb945929c518d",
      "4c0e5330ad694e028ed0b5bca28e9e0e",
      "2ae382b94956478691b59f775e53bb8e",
      "8b8757eaad904f729aa2ab97e6c57814",
      "0af8066db7de4ffdaa71b84e04101070",
      "8c99b747811348f89524f941fe522324"
     ]
    },
    "id": "qOYMKjhqsx8s",
    "outputId": "180eae98-af4a-41e5-ff0f-42bb8b958676"
   },
   "outputs": [],
   "source": [
    "def acc_metric(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    return (input.argmax(dim=1) == target).float().mean()\n",
    "\n",
    "\n",
    "wd = 1e-2\n",
    "lr = 1e-3\n",
    "arch = models.resnet34\n",
    "learn = unet_learner(data,\n",
    "                     arch,\n",
    "                     metrics=acc_metric,\n",
    "                     wd=wd)\n",
    "\n",
    "# early stopping:\n",
    "#cbs=[EarlyStoppingCallback(learn, monitor='valid_loss', min_delta=0.1, patience=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "7XqinU3usx8s",
    "outputId": "b9051718-7843-4aff-ad0f-ff6957142672"
   },
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLUzz4nXsx8s"
   },
   "source": [
    "### First cycle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLMnRxfbsx8s"
   },
   "outputs": [],
   "source": [
    "sim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hw1170CUsx8s"
   },
   "outputs": [],
   "source": [
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "Z-D0Nzprsx8s",
    "outputId": "cc9bac2d-d309-49ed-aebe-32ef65c37ca2"
   },
   "outputs": [],
   "source": [
    "do_fit('sim{}_1a'.format(sim), slice(lr * 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "-ZjA9eYxsx8s",
    "outputId": "a291278f-73fc-404e-b67e-abf1e1aa265d",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "img = learn.data.valid_ds.x[3]\n",
    "mask = learn.data.valid_ds.y[3]\n",
    "pred = learn.predict(img)[0]\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 6))\n",
    "img.show(ax[0])\n",
    "mask.show(ax[1])\n",
    "pred.show(ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtobX7HSscHm",
    "outputId": "4559ef3f-5678-4104-a72b-d1230557e6c4"
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "for i in tqdm(range(len(learn.data.valid_ds.x))):\n",
    "  img = learn.data.valid_ds.x[i]\n",
    "  mask = learn.data.valid_ds.y[i]\n",
    "  pred = learn.predict(img)[0]\n",
    "  label_mask = patch_to_label(mask.data.numpy())\n",
    "  y_true.append(label_mask)\n",
    "  label_pred = patch_to_label(pred.data.numpy())\n",
    "  y_pred.append(label_pred)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFlLWbJjvImv",
    "outputId": "df8ac638-bcd1-4714-c42d-6e9a0ed090c9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lHzwLXfasx8s",
    "outputId": "7d87ff80-ae46-49cb-b4e0-925182fb4cae",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(20, 3, figsize=(10, 20))\n",
    "for i in range(20):\n",
    "    img = learn.data.valid_ds.x[i]\n",
    "    mask = learn.data.valid_ds.y[i]\n",
    "    pred = learn.predict(img)[0]\n",
    "    img.show(ax[i, 0])\n",
    "    mask.show(ax[i, 1])\n",
    "    label_mask = patch_to_label(mask.data.numpy())\n",
    "    ax[i,1].set_title(f'label:{label_mask}')\n",
    "    pred.show(ax[i, 2])\n",
    "    label_pred = patch_to_label(pred.data.numpy())\n",
    "    ax[i,2].set_title(f'label:{label_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhz8Ol6vsx8t"
   },
   "source": [
    "## Create predictions :\n",
    "Your predictions must be in .csv format, see sample-submission.csv. You must use the same datapoint ids as in the test set test.csv. To generate .csv output from Python, use our provided helper functions in helpers.py (see Project 1 folder on github)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uk7IPaVTsx8t"
   },
   "outputs": [],
   "source": [
    "if not (Path('data/test_set_images/all_tests')).exists():\n",
    "    (Path('data/test_set_images/all_tests')).mkdir()\n",
    "\n",
    "if not (Path('data/test_set_images/all_tests/patches')).exists():\n",
    "    (Path('data/test_set_images/all_tests/patches')).mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOPztH2wsx8t",
    "outputId": "8f11c631-e1f2-4c69-8e08-4fbb655579c2"
   },
   "outputs": [],
   "source": [
    "! find data/test_set_images -type f -name \"*.png\" -exec mv \"{}\" data/test_set_images/all_tests \\;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2QOn6Dbsx8t"
   },
   "outputs": [],
   "source": [
    "path_test = Path('data/test_set_images/all_tests')\n",
    "fnames_test = get_image_files(path_test)\n",
    "path_test_patches = Path('data/test_set_images/all_tests/patches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jSOAPjKsx8t"
   },
   "source": [
    "### Pre-process test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UT0f3r00sx8t"
   },
   "source": [
    "Create the list of submission names (for each test patch of 16x16 top left coordinate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wpp9Ilhzsx8t",
    "outputId": "10208bef-6657-4d52-e5c2-fb2b71a9eb9b"
   },
   "outputs": [],
   "source": [
    "fnames_test_patches = []\n",
    "test_img = open_image(fnames_test[0])\n",
    "for img_number in range(1,51):\n",
    "    for j in range(0, test_img.shape[1], patch_size):\n",
    "            for i in range(0,  test_img.shape[2], patch_size):\n",
    "                name = \"{:03d}_{}_{}.png\".format(img_number, j, i)\n",
    "                fnames_test_patches.append(name)\n",
    "len(fnames_test_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlcjDHwhsx8t"
   },
   "source": [
    "Create test patches and save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWcQ19zmsx8t"
   },
   "outputs": [],
   "source": [
    "def create_test_patches(test_file_name):\n",
    "    img_number = int(re.search(r\"\\d+\", str(test_file_name)).group(0))\n",
    "    im = mpimg.imread(Path(test_file_name))\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[j:j + patch_size, i:i + patch_size]\n",
    "            rgb_patch = Image.fromarray((256*patch).astype(np.uint8), 'RGB')\n",
    "            name = \"{:03d}_{}_{}.png\".format(img_number, j, i)\n",
    "            rgb_patch.save(path_test_patches/name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCfvfE7Lsx8t",
    "outputId": "e4b8808a-41bd-4acc-bbb3-8c3b973dbbff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(fnames_test))): \n",
    "    test_im = fnames_test[i]\n",
    "    create_test_patches(test_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIJ2sompsx8t",
    "outputId": "486b55fe-7301-44a1-ce76-25ef4d21d471",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "print(f'Number of created test patches: {len(get_image_files(path_test_patches))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIbEc9Tesx8t"
   },
   "source": [
    "### Predict labels for patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liyd0VLOsx8t"
   },
   "outputs": [],
   "source": [
    "# assign a label to a patch\n",
    "foreground_threshold = 0.25\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVn9UJW_sx8t",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "\n",
    "def label_predictions(test_file_name, path_test_patches):\n",
    "    img_number = int(re.search(r\"\\d+\", str(test_file_name)).group(0))\n",
    "    # prediction from patch:\n",
    "    tensor = open_image(path_test_patches / test_file_name)\n",
    "    prediction = learn.predict(tensor)\n",
    "    label = patch_to_label(prediction[0].data.numpy())\n",
    "    name = test_file_name[:-4]\n",
    "    yield (\"{},{}\".format(name, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67PUDu-nsx8u"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def masks_to_submission(submission_filename, image_filenames,\n",
    "                        path_test_patches):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for i in tqdm(range(len(image_filenames))):\n",
    "            fn = image_filenames[i]\n",
    "            f.writelines('{}\\n'.format(s)\n",
    "                         for s in label_predictions(fn, path_test_patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xh3wp_Xxsx8u",
    "outputId": "f7f9c842-4e16-4f53-fa22-2a22854068d6"
   },
   "outputs": [],
   "source": [
    "#masks_to_submission('sim{}_1a.csv'.format(sim), fnames_test_patches, path_test_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5mevf4DBfMP"
   },
   "outputs": [],
   "source": [
    "def concatenate_mask_test(list_masks):\n",
    "    \"\"\"list_masks of size (?, 16, 16) where the image \n",
    "    is assembled column after column so first 38 elements \n",
    "    are the first elements in the first column starting from pos [0,0] to [38,0]\n",
    "    masks are numpy of size (16,16)\"\"\"\n",
    "    z = np.zeros((608, 608))\n",
    "    for i in range(38):\n",
    "        rows = np.concatenate(list_masks[0 + i * 38:38 + i * 38], axis=1)\n",
    "        z[0 + i * 16:16 + i * 16,:] = rows\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WIrU-XPCrYy"
   },
   "source": [
    "### Predict a few tests: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "JS7pJtYlsx8u",
    "outputId": "62c147b6-2be6-4475-981c-e8b625f13d40"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 2, figsize=(10, 10))\n",
    "\n",
    "for m in range(5):\n",
    "    fnames_test_1 = []\n",
    "    test_img = open_image(fnames_test[m])\n",
    "    img_number = int(re.search(r\"\\d+\", str(fnames_test[m])).group(0))\n",
    "\n",
    "    for j in range(0, test_img.shape[1], patch_size):\n",
    "        for i in range(0, test_img.shape[2], patch_size):\n",
    "            name = \"{:03d}_{}_{}.png\".format(img_number, j, i)\n",
    "            fnames_test_1.append(name)\n",
    "\n",
    "    predictions = []\n",
    "    for j in tqdm(range(len(fnames_test_1))):\n",
    "        img = fnames_test_1[j]\n",
    "        img_number = int(re.search(r\"\\d+\", str(img)).group(0))\n",
    "        # prediction from patch:\n",
    "        tensor = open_image(path_test_patches / img)\n",
    "        prediction = learn.predict(tensor)\n",
    "        predictions.append(prediction[0].data.numpy()[0, :, :])\n",
    "        #label = patch_to_label(prediction[0].data.numpy())\n",
    "    axs[m, 0].imshow(mpimg.imread(fnames_test[m]))\n",
    "    axs[m, 1].imshow(concatenate_mask_test(predictions), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTAMg4JLDCmr"
   },
   "source": [
    "## Create submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed_JEyrHyQJ2"
   },
   "source": [
    "### Mask to submission:\n",
    "Use given py to create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ket6zvB0yUlg"
   },
   "outputs": [],
   "source": [
    "def mask_to_submission_strings_whole(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    image_filename = str(image_filename)\n",
    "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = 16\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i:i + patch_size, j:j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "            \n",
    "def masks_to_submission_whole(submission_filename, image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for i in tqdm(range(len(image_filenames))):\n",
    "          fn = image_filenames[i]\n",
    "          f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings_whole(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LAohZQHDQku"
   },
   "source": [
    "Create whole masks for all test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUUqJYCHIUuD"
   },
   "outputs": [],
   "source": [
    "path_predictions = Path('data/test_set_images/all_tests/predictions')\n",
    "if not (path_predictions).exists():\n",
    "    (Path(path_predictions)).mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPA9-cN-yaXM",
    "outputId": "11dcd851-59e1-4aa3-f51f-c819184861ab"
   },
   "outputs": [],
   "source": [
    "def masks_from_tests(fnames_test):\n",
    "  for m in tqdm(range(len(fnames_test))):\n",
    "      fnames_test_1 = []\n",
    "      test_img = open_image(fnames_test[m])\n",
    "      img_number = int(re.search(r\"\\d+\", str(fnames_test[m])).group(0))\n",
    "\n",
    "      for j in range(0, test_img.shape[1], patch_size):\n",
    "          for i in range(0, test_img.shape[2], patch_size):\n",
    "              name = \"{:03d}_{}_{}.png\".format(img_number, j, i)\n",
    "              fnames_test_1.append(name)\n",
    "\n",
    "      predictions = []\n",
    "      for j in (range(len(fnames_test_1))):\n",
    "          img = fnames_test_1[j]\n",
    "          img_number = int(re.search(r\"\\d+\", str(img)).group(0))\n",
    "          # prediction from patch:\n",
    "          tensor = open_image(path_test_patches / img)\n",
    "          prediction = learn.predict(tensor)\n",
    "          predictions.append(prediction[0].data.numpy()[0, :, :])\n",
    "\n",
    "      whole_mask = concatenate_mask_test(predictions)\n",
    "      im_mask = Image.fromarray((np.where(whole_mask > 0.5, 1, 0) * 255).astype(np.uint8),\n",
    "                            'L')\n",
    "      name = \"{:03d}.png\".format(img_number)\n",
    "      im_mask.save(path_predictions/name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZoLSM3DDhDV"
   },
   "outputs": [],
   "source": [
    "masks_from_tests(fnames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3Kuwnlsze2W"
   },
   "outputs": [],
   "source": [
    "path_predictions = Path('data/test_set_images/all_tests/predictions')\n",
    "fnames_predictions = get_image_files(path_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_f-4rKDDk4y"
   },
   "source": [
    "Plot a few masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "zY2rVLDR2oLC",
    "outputId": "b57c4ca6-836f-4df5-bfa8-40ad89f90cb4"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(10,2, figsize = (10, 15))\n",
    "m = 20\n",
    "for i in range(10):\n",
    "  for j in range(2):\n",
    "    axs[i,j].imshow(mpimg.imread(fnames_predictions[m]), cmap='Greys_r')\n",
    "    m+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPRuxlHL2LIs",
    "outputId": "2e21ba8f-c603-4ede-f0b8-6878c723f607"
   },
   "outputs": [],
   "source": [
    "masks_to_submission_whole('sim{}_1a.csv'.format(4), fnames_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRP4f28n04o8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "project2_ML.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0af8066db7de4ffdaa71b84e04101070": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c82447708ad49cb9db6c0731f2e6e89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ae382b94956478691b59f775e53bb8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4c0e5330ad694e028ed0b5bca28e9e0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c99b747811348f89524f941fe522324",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0af8066db7de4ffdaa71b84e04101070",
      "value": " 83.3M/83.3M [00:18&lt;00:00, 4.72MB/s]"
     }
    },
    "8b8757eaad904f729aa2ab97e6c57814": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c99b747811348f89524f941fe522324": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d8884f3a2e3401485e67ec7b8f87a7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9af9506386cd4d94b2dcb945929c518d",
       "IPY_MODEL_4c0e5330ad694e028ed0b5bca28e9e0e"
      ],
      "layout": "IPY_MODEL_1c82447708ad49cb9db6c0731f2e6e89"
     }
    },
    "9af9506386cd4d94b2dcb945929c518d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b8757eaad904f729aa2ab97e6c57814",
      "max": 87306240,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ae382b94956478691b59f775e53bb8e",
      "value": 87306240
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
